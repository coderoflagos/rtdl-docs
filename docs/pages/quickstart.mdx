---
title: Quickstart
description: rtdl quickstart instructions
---

# Quickstart ðŸŒ±
## Initialize the rtdl services
1.  Run `docker compose -f docker-compose.init.yml up -d`.
    * **Note:** This configuration should be fault-tolerant, but if any containers or 
      processes fail when running this, run `docker compose -f docker-compose.init.yml down` 
      and retry.
2.  After the containers `rtdl_rtdl-db-init` and `rtdl_dremio-init` exit and complete with `EXITED (0)`, kill and 
    delete the rtdl container set by running `docker compose -f docker-compose.init.yml down`.
3.  Run `docker compose up -d` every time after.  
    **Note:** Your memory setting in Docker must be at least 8GB. rtdl may become unstable if it is 
    set lower.
    * `docker compose down` to stop.

## Interact with rtdl services and create a data lake
All API calls used to interact with rtdl have Postman examples in our [postman-rtdl-public repo](https://github.com/realtimedatalake/postman-rtdl-public).
1.  If you are building your data lake on a cloud vendor's storage service, configure your storage 
    buckets and access:
    * For AWS S3, follow the [Segment docs for AWS S3](https://segment.com/docs/connections/storage/catalog/aws-s3/). 
      You will need your bucket name, your AWS access key id, and your AWS secret access key.
      * For your IAM setup, you can use the below policy:
        ```
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Sid": "ListAllBuckets",
                    "Effect": "Allow",
                    "Action": [
                        "s3:GetBucketLocation",
                        "s3:ListAllMyBuckets"
                    ],
                    "Resource": [
                        "arn:aws:s3:::*"
                    ]
                },
                {
                    "Sid": "ListBucket",
                    "Effect": "Allow",
                    "Action": [
                        "s3:ListBucket"
                    ],
                    "Resource": [
                        "arn:aws:s3:::rtdl-test-bucket-aws"
                    ]
                },
                {
                    "Sid": "ManageBucket",
                    "Effect": "Allow",
                    "Action": [
                        "s3:GetObject",
                        "s3:PutObject",
                        "s3:PutObjectAcl",
                        "s3:DeleteObject"
                    ],
                    "Resource": [
                        "arn:aws:s3:::rtdl-test-bucket-aws/*"
                    ]
                }
            ]
        }
        ```
    * For GCP Cloud Storage, follow the [Segment docs for Google Cloud Storage](https://segment.com/docs/connections/storage/catalog/google-cloud-storage/). 
      Instead of giving your service account object-level access as described in Segment's 
      documentation, make your service account a Principal in IAM and give it `Storage Admin` access.
      * You will need your credentials in flattened json (remove all the newlines).
3.  Instrument your website with [analytics-next-cc](https://github.com/realtimedatalake/analytics-next-cc) - 
    our fork of [Segment's Analytics.js 2.0](https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/) 
    that lets you cc all of the events you send to Segment to rtdl's ingest endpoint. Its 
    snippet is a drop-in replacement of Analytics.js 2.0/Analytics.js. Using this makes it 
    easy to build your data lake with existing Segment instrumentation. Enter your ingest endpoint
    as the `ccUrl` value and rtdl will handle the payload. Make sure you enter your writeKey in the 
    `stream_alt_id` of your `stream` configuration (below).
    * Alternatively, you can send ***any*** json with just ```stream_id``` in the payload and rtdl will add it to your lake.
      ```
      {
          "stream_id":"837a8d07-cd06-4e17-bcd8-aef0b5e48d31",
          "name":"user1",
          "array":[1,2,3],
          "properties":{"age":20}
      }
      ```
	You can optionally add ```message_type``` should you choose to override the ```message_type``` specified while creating the stream.
	rtdl will default to a message type ```rtdl_default``` if message type is absent in both stream definition and actual message
	
	
4.  Create/read/update/delete `stream` configurations that define a source data stream into 
    your data lake and the destination data store as well as configure folder partitioning and 
    file compression. It also allows for activating/deactivating a stream.
    * For any json data being sent to the ingest endpoint, the generated `stream_id` or the 
      manually input `stream_alt_id` values are required in the payload.

**Note:** To start from scratch, run `rm -rf storage/` from the rtdl root folder.
