---
title: Quickstart
description: rtdl quickstart instructions
---

# Quickstart
## Initialize the rtdl services
1.  Run `docker compose -f docker-compose.init.yml up -d`.
    * **Note:** This configuration should be fault-tolerant, but if any containers or 
      processes fail when running this, run `docker compose -f docker-compose.init.yml down` 
      and retry.
2.  After the containers `rtdl_rtdl-db-init` and `rtdl_dremio-init` exit and complete with `EXITED (0)`, kill and 
    delete the rtdl container set by running `docker compose -f docker-compose.init.yml down`.
3.  Run `docker compose up -d` every time after.  
    **Note:** Your memory setting in Docker must be at least 8GB. rtdl may become unstable if it is 
    set lower.
    * `docker compose down` to stop.

## Interact with rtdl services and create a data lake
All API calls used to interact with rtdl have Postman examples in our [postman-rtdl-public repo](https://github.com/realtimedatalake/postman-rtdl-public).
1.  If you are building your data lake on AWS or GCP, configure your storage buckets and access 
    by following the [Segment docs for AWS S3](https://segment.com/docs/connections/storage/catalog/aws-s3/) 
    or the [Segment docs for Google Cloud Storage](https://segment.com/docs/connections/storage/catalog/google-cloud-storage/). 
    * For AWS S3 storage, you will need your bucket name, your AWS access key id, and your AWS
      secret access key.
      * For your IAM setup, you can use the below policy:
        ```
        {
            "Version": "2012-10-17",
            "Statement": [
                {
              "Sid": "PutObjectsInBucket",
              "Effect": "Allow",
              "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:DeleteObject",
                "s3:ListBucket"
              ],
              "Resource": [
                "arn:aws:s3:::[your-bucket-name]/",
                "arn:aws:s3:::[your-bucket-name]/*"
              ]
            }
          ]
        }
        ```
    * For GCP Cloud Storage, you will need your credentials in flattened json (remove all the 
      newlines).

2.  Instrument your website with [analytics-next-cc](https://github.com/realtimedatalake/analytics-next-cc) - 
    our fork of [Segment's Analytics.js 2.0](https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/) 
    that lets you cc all of the events you send to Segment to rtdl's ingest endpoint. Its 
    snippet is a drop-in replacement of Analytics.js 2.0/Analytics.js. Using this makes it 
    easy to build your data lake with existing Segment instrumentation. Enter your ingest endpoint
    as the `ccUrl` value and rtdl will handle the payload. Make sure you enter your writeKey in the 
    `stream_alt_id` of your `stream` configuration (below).
    * Alternatively, you can send ***any*** json with just ```stream_id``` in the payload and rtdl will add it to your lake.
      ```
      {
          "stream_id":"837a8d07-cd06-4e17-bcd8-aef0b5e48d31",
          "name":"user1",
          "array":[1,2,3],
          "properties":{"age":20}
      }
      ```
	You can optionally add ```message_type``` should you choose to override the ```message_type``` specified while creating the stream.
	rtdl will default to a message type ```rtdl_default``` if message type is absent in both stream definition and actual message
	
	
3.  Create/read/update/delete `stream` configurations that define a source data stream into 
    your data lake and the destination data store as well as configure folder partitioning and 
    file compression. It also allows for activating/deactivating a stream.
    * For any json data being sent to the ingest endpoint, the generated `stream_id` or the 
      manually input `stream_alt_id` values are required in the payload.

**Note:** To start from scratch, run `rm -rf storage/` from the rtdl root folder.
